# -*- coding: utf-8 -*-
"""Stage_3_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KlBjTDY-uXQleqIYy1gdFi4rl42HBurB

#Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import glob

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor

from sklearn.gaussian_process.kernels import RBF, DotProduct
from sklearn.model_selection import cross_validate

from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import cross_val_score

"""#Combined Data Prediction"""

# Read the dataset
df = pd.read_csv('/content/Combined Dataset - Updated.csv')
dfn = pd.read_csv('/content/Combined Dataset - Updated.csv', usecols=['cycle number','Temperature','Q discharge/mA.h'])

# Assign variables x, T, and y
x = dfn['cycle number']
T = dfn['Temperature']
y = dfn['Q discharge/mA.h']

array1 = np.power(x,4)
array2 = np.power(x,3)
array3 = np.power(x,2)
array4 = np.power(x,1.5)
array5 = np.power(x,1)
array6 = (np.exp(-100/T)+T)*x
z = combined_array = np.concatenate((array1[:, np.newaxis], array2[:, np.newaxis],array3[:, np.newaxis],array4[:, np.newaxis],array5[:, np.newaxis],array6[:, np.newaxis]), axis=1)

x = z
y = dfn.iloc[:, -2].values

# Divide the dataset into train, test, and validation sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=4)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=4)

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_val_scaled = scaler.transform(X_val)

random_indices = np.random.choice(len(X_test_scaled), size=10, replace=False)

# Select values at the randomly chosen indices from both arrays
X_test_scaled_1 = X_test_scaled[random_indices]
y_test_1 = y_test[random_indices]

from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel, Matern, RationalQuadratic, ExpSineSquared

# Define the parameter grid
param_grid = {
    "alpha": [0.001, 0.002, 0.003],
    "kernel": [
        RBF(length_scale=l) for l in np.logspace(1, 2)] +
        [RationalQuadratic(length_scale=2.0, alpha=0.01)],
    "n_restarts_optimizer": [1, 2, 3],
    "normalize_y": [True, False]
}

# Instantiate the grid search model
grid_search = GridSearchCV(
    estimator=GaussianProcessRegressor(),
    param_grid=param_grid,
    cv=2,
    n_jobs=-1,
    verbose=2
)
grid_search.fit(X_train_scaled, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
print("best_params :", best_params)
print("best_model :", best_model)

y_pred_train = best_model.predict(X_train_scaled)
y_pred_test = best_model.predict(X_test_scaled)
y_pred_val = best_model.predict(X_val_scaled)
y_pred_test_1 = best_model.predict(X_test_scaled_1)

# Calculate evaluation metrics on the train set
mse_train = mean_squared_error(y_train, y_pred_train)
mae_train = mean_absolute_error(y_train, y_pred_train)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, y_pred_train)

print("Train set - MSE:", mse_train, "MAE:", mae_train, "RMSE:", rmse_train, "R2:", r2_train)

# Calculate evaluation metrics on the test set
mse_test = mean_squared_error(y_test, y_pred_test)
mae_test = mean_absolute_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)
r2_test = r2_score(y_test, y_pred_test)

print("Test set - MSE:", mse_test, "MAE:", mae_test, "RMSE:", rmse_test, "R2:", r2_test)

# Calculate evaluation metrics on the validation set
mse_val = mean_squared_error(y_val, y_pred_val)
mae_val = mean_absolute_error(y_val, y_pred_val)
rmse_val = np.sqrt(mse_val)
r2_val = r2_score(y_val, y_pred_val)

print("Validation set - MSE:", mse_val, "MAE:", mae_val, "RMSE:", rmse_val, "R2:", r2_val)

mse_test_1 = mean_squared_error(y_test_1, y_pred_test_1)
mae_test_1 = mean_absolute_error(y_test_1, y_pred_test_1)
rmse_test_1 = np.sqrt(mse_test_1)
r2_test_1 = r2_score(y_test_1, y_pred_test_1)
print("Test set 1 - MSE:", mse_test_1, "MAE:", mae_test_1, "RMSE:", rmse_test_1, "R2:", r2_test_1)

import pandas as pd
import numpy as np

# Assuming you have the y_test and y_pred_test values
# Convert to numpy arrays for easier handling
y_train_np = np.array(y_train)
y_pred_train_np = np.array(y_pred_train)

# Find indices of non-NaN values
non_nan_indices = ~np.isnan(y_train_np) & ~np.isnan(y_pred_train_np)

# Create a DataFrame with non-NaN values
result_df = pd.DataFrame({
    'Actual': y_train_np[non_nan_indices],
    'Predictions': y_pred_train_np[non_nan_indices]
})

# Set display options to show all rows
pd.set_option('display.max_rows', None)

# Print the resulting DataFrame
print(result_df)

# Reset display options to default
pd.reset_option('display.max_rows')

import pandas as pd
import numpy as np

# Assuming you have the y_test and y_pred_test values
# Convert to numpy arrays for easier handling
y_test_np = np.array(y_test)
y_pred_test_np = np.array(y_pred_test)

# Find indices of non-NaN values
non_nan_indices = ~np.isnan(y_test_np) & ~np.isnan(y_pred_test_np)

# Create a DataFrame with non-NaN values
result_df = pd.DataFrame({
    'Actual': y_test_np[non_nan_indices],
    'Predictions': y_pred_test_np[non_nan_indices]
})

# Set display options to show all rows
pd.set_option('display.max_rows', None)

# Print the resulting DataFrame
print(result_df)

# Reset display options to default
pd.reset_option('display.max_rows')

import pandas as pd
import numpy as np

# Assuming you have the y_test and y_pred_test values
# Convert to numpy arrays for easier handling
y_val_np = np.array(y_val)
y_pred_val_np = np.array(y_pred_val)

# Find indices of non-NaN values
non_nan_indices = ~np.isnan(y_val_np) & ~np.isnan(y_pred_val_np)

# Create a DataFrame with non-NaN values
result_df = pd.DataFrame({
    'Actual': y_val_np[non_nan_indices],
    'Predictions': y_pred_val_np[non_nan_indices]
})

# Set display options to show all rows
pd.set_option('display.max_rows', None)

# Print the resulting DataFrame
print(result_df)

# Reset display options to default
pd.reset_option('display.max_rows')

# Generate row numbers
row_numbers_train = [i for i in range(1, len(y_pred_train) + 1)]
row_numbers_test = [i for i in range(1, len(y_pred_test) + 1)]
row_numbers_val = [i for i in range(1, len(y_pred_val) + 1)]

# Scatter plot for train set
plt.figure(figsize=(8, 6))
plt.scatter(row_numbers_train, y_train, color='blue', label='Actual')
plt.scatter(row_numbers_train, y_pred_train, color='orange', label='Predicted')
plt.xlabel('Sample')
plt.ylabel('Target Value')
plt.title('Train Set - Actual vs Predicted')
plt.legend()
plt.show()

# Scatter plot for test set
plt.figure(figsize=(8, 6))
plt.scatter(row_numbers_test, y_test, color='green', label='Actual')
plt.scatter(row_numbers_test, y_pred_test, color='orange', label='Predicted')
plt.xlabel('Sample')
plt.ylabel('Target Value')
plt.title('Test Set - Actual vs Predicted')
plt.legend()
plt.show()

# Scatter plot for validation set
plt.figure(figsize=(8, 6))
plt.scatter(row_numbers_val, y_val, color='red', label='Actual')
plt.scatter(row_numbers_val, y_pred_val, color='orange', label='Predicted')
plt.xlabel('Sample')
plt.ylabel('Target Value')
plt.title('Validation Set - Actual vs Predicted')
plt.legend()
plt.show()